name: RL_Agents

# ======================== Agent's Hyperparameter ========================
# The initial hyperparameters of the population
hp_initial:
  lr: 0.001
  gamma: 0.9
  epsilon: 0.05
  strength_mutation: 0.1
# The name of the exploration strategy, among epsilon_greedy, softmax, and greedy
name_exploration: softmax
# The size of the replay buffer
size_replay_buffer: 100000
# The batch size
batch_size: 4096
# The number of gradient steps to perform at each episode
n_gradient_steps: 10
# The maximum absolute value allowed for gradients. They will be clipped as such : grad = clip(grad, -value_gradient_clipping, value_gradient_clipping)
value_gradient_clipping: 4.0

# ======================== Evolution Hyperparameters ========================
mode_weights_transmission: final # initial, final, none

# ======================== Models Config ========================

# The configuration of the reward model
reward_model:
  func_weight: hardcoded # constant, linear, hardcoded, one
  dict_reward:
    energy: ${eval:'${env.energy_max} / (${env.energy_food} - 1)'} # set the reward of eating a plant to 1
    age: 0
    n_childrens: 0

# Whether to use an sensor model. If True, the agent will use the config's model as the sensor model. If False, the agent will directly use the observation as input for the decision model.
do_sensor_model: False

# The dimension of the latent space corresponding to the internal representation of the observation
n_sensations: 11

# The configuration of the decision model
decision_model:
  hidden_dims: []

# ======================== Defaults sub-configs and other Hydra config ========================
defaults:
  - _self_
  - metrics : base_agents_metrics